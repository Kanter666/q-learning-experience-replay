{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experience Replay\n",
    "\n",
    "In this project we study the influence of experience replay to improve Q-Learning.\n",
    "We study the following experience replay techniques,\n",
    "- Uniform ER\n",
    "- Prioritized ER\n",
    "- Hindsight ER\n",
    "- Combined ER\n",
    "\n",
    "Our experiements are limited to a subset of the Open-AI Gym's Classic Control environments,\n",
    "- Cart Pole\n",
    "- Mountain Car\n",
    "- Acrobot\n",
    "\n",
    "Using these ER methods for Q-Learning in the Gym environments, we measure the influence of i.i.d. and TD-error to quantiatively compare the performances.\n",
    "\n",
    "Our hypothesis is that the effect of i.i.d. has a greater influence in the performance in comparision to the TD-error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "In this section we load the required libraries to implement Q-Learning and run the environments of Open-AI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-37e6a4e0ac24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Here, we define a Neural Network for the Q function. The forward pass accepts the observation state and outputs the action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=4, num_hidden=128, output_dim=2):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(input_dim, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l2(self.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "input_dim = 4\n",
    "num_hidden = 128\n",
    "output_dim = 2\n",
    "torch.manual_seed(1234)\n",
    "model = QNetwork(input_dim, num_hidden, output_dim)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(input_dim, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, output_dim)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# at this point we do not need backpropagation\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(model(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions for training,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epsilon(it):\n",
    "    if it == 0:\n",
    "        return 1\n",
    "    return np.maximum(1 - it/1000, 0.05)\n",
    "\n",
    "def select_action(model, state, epsilon):\n",
    "    with torch.no_grad():\n",
    "        actionProbs = model(torch.Tensor(state)).numpy()\n",
    "    action = np.random.choice(len(actionProbs), 1) if np.random.rand() < epsilon else np.argmax(actionProbs)\n",
    "    return int(action.squeeze())\n",
    "\n",
    "def copy_model(source_model, target_model):\n",
    "    target_model.load_state_dict(source_model.state_dict())\n",
    "\n",
    "def append_sample(transition, model, target_model, discount_factor, memory):\n",
    "    state, action, reward, next_state, done = transition\n",
    "    target = model(torch.FloatTensor(state)).data\n",
    "    old_val = target[0][action]\n",
    "    target_val = target_model(torch.FloatTensor(next_state)).data\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + discount_factor * torch.max(target_val)\n",
    "\n",
    "    error = abs(old_val - target[0][action])\n",
    "\n",
    "    memory.push((state, action, reward, next_state, done), error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(memory, model, target_model, batch_size, action_size, optimizer, discount_factor):\n",
    "\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    mini_batch, idxs, is_weights = memory.sample(batch_size)\n",
    "    mini_batch = np.array(mini_batch).transpose()\n",
    "\n",
    "    states = np.vstack(mini_batch[0])\n",
    "    actions = list(mini_batch[1])\n",
    "    rewards = list(mini_batch[2])\n",
    "    next_states = np.vstack(mini_batch[3])\n",
    "    dones = mini_batch[4]\n",
    "\n",
    "    # bool to binary\n",
    "    dones = dones.astype(int)\n",
    "\n",
    "    # Q function of current state\n",
    "    states = torch.Tensor(states)\n",
    "    pred = model(states)\n",
    "\n",
    "    # one-hot encoding\n",
    "    a = torch.LongTensor(actions).view(-1, 1)\n",
    "\n",
    "    one_hot_action = torch.zeros(batch_size, action_size)\n",
    "    one_hot_action.scatter_(1, a, 1)\n",
    "\n",
    "    pred = torch.sum(pred.mul(torch.tensor(one_hot_action)), dim=1)\n",
    "\n",
    "    # Q function of next state\n",
    "    next_states = torch.Tensor(next_states)\n",
    "    next_pred = target_model(next_states).data\n",
    "\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    dones = torch.FloatTensor(dones)\n",
    "\n",
    "    # Q Learning: get maximum Q value at s' from target model\n",
    "    target = rewards + (1 - dones) * discount_factor * next_pred.max(1)[0]\n",
    "    target = torch.autograd.Variable(target)\n",
    "\n",
    "    errors = torch.abs(pred - target).data.numpy()\n",
    "\n",
    "    # update priority\n",
    "    if isinstance(memory, PrioritizedMemory):\n",
    "        for i in range(batch_size):\n",
    "            idx = idxs[i]\n",
    "            memory.update(idx, errors[i])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # MSE Loss function\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    loss.backward()\n",
    "\n",
    "    # and train\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episodes(train, model, target_model, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "\n",
    "    durations = torch.zeros(num_episodes)\n",
    "    iteration = 0\n",
    "    \n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(model, state, get_epsilon(iteration))\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -10\n",
    "\n",
    "            transition = (state, action, reward, next_state, done)\n",
    "\n",
    "            append_sample(transition, model, target_model, discount_factor, memory)\n",
    "\n",
    "            train(memory, model, target_model, batch_size, action_size, optimizer, discount_factor)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            durations[episode] += 1\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                copy_model(model, target_model)\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 10\n",
    "                scores.append(score)\n",
    "    return durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "We load the necessary Gym environments required for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment \t\t State Space \t Action Space\n",
      "----------- \t\t ----------- \t ------------\n",
      "CartPole-v0 \t\t Box(4,) \t Discrete(2)\n",
      "Acrobot-v1 \t\t Box(6,) \t Discrete(3)\n",
      "MountainCar-v0 \t\t Box(2,) \t Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "environments = [\"CartPole-v0\", \"Acrobot-v1\", \"MountainCar-v0\"]\n",
    "envs = []\n",
    "\n",
    "print('Environment \\t\\t State Space \\t Action Space')\n",
    "print('----------- \\t\\t ----------- \\t ------------')\n",
    "for environment in environments:\n",
    "    env = gym.envs.make(environment)\n",
    "    envs.append(env)\n",
    "    print('{} \\t\\t {} \\t {}'.format(\n",
    "        environment,\n",
    "        env.observation_space,\n",
    "        env.action_space)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition, error=None): # extra None to have the same behaviour as PER\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        raise NotImplementedError('Function sample is not defined')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Experience Replay and Uniform Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoneMemory(ReplayMemory):\n",
    "    def sample(self, batch_size):\n",
    "        return self.memory[-batch_size:], None, None # extra Nones to have the same behaviour as PER\n",
    "\n",
    "\n",
    "class UniformMemory(ReplayMemory):\n",
    "    def sample(self, batch_size):\n",
    "        index = np.random.choice(len(self), batch_size, replace=False)\n",
    "        return [self.memory[i] for i in index], None, None # extra Nones to have the same behaviour as PER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/rlcode/per\n",
    "\n",
    "# SumTree\n",
    "# a binary tree data structure where the parent’s value is the sum of its children\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "\n",
    "class PrioritizedMemory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "    def push(self, sample, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the memory module works as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.4097208,  0.       ]), 0, -1.0, array([-0.41155836, -0.00183757]), False)\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = NoneMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the train module works as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7917559742927551\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "\n",
    "model = QNetwork(input_dim, num_hidden, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "\n",
    "# setup memory\n",
    "transition = memory.sample(1)[0][0]\n",
    "memory = UniformMemory(10 * batch_size)\n",
    "\n",
    "# fill with dummy data\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# must print a loss\n",
    "print(train(memory, model, model, batch_size, output_dim, optimizer, discount_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(er_type='uniform'):\n",
    "    # load environment\n",
    "    env = envs[0]\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    num_hidden = 128\n",
    "\n",
    "    # init networks\n",
    "    model = QNetwork(state_size, num_hidden, action_size)\n",
    "    target_model = QNetwork(state_size, num_hidden, action_size)\n",
    "\n",
    "    copy_model(model, target_model)\n",
    "\n",
    "    # run settings\n",
    "    batch_size = 64\n",
    "    discount_factor = 0.8\n",
    "    learn_rate = 1e-3\n",
    "    memory_size = 10000\n",
    "    num_episodes = 200\n",
    "    \n",
    "    er_types = {\n",
    "        'none': NoneMemory,\n",
    "        'uniform': UniformMemory,\n",
    "        'prioritized': PrioritizedMemory\n",
    "    }\n",
    "\n",
    "    memory = er_types[er_type](memory_size)\n",
    "    seed = 42  # This is not randomly chosen\n",
    "\n",
    "    # We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    episode_durations = run_episodes(train, model, target_model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:10<00:00, 19.38it/s]\n",
      " 69%|██████▉   | 138/200 [00:16<00:13,  4.47it/s]"
     ]
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "runs = {\n",
    "    'none': 'No ER',\n",
    "    'uniform': 'Uniform ER',\n",
    "    'prioritized': 'Prioritized ER'\n",
    "}\n",
    "\n",
    "for er_type in runs:\n",
    "    plt.plot(smooth(run(er_type), 10))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend([runs[er_type] for er_type in runs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
