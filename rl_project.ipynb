{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experience Replay\n",
    "\n",
    "In this project we study the influence of experience replay to improve Q-Learning.\n",
    "We study the following experience replay techniques,\n",
    "- Uniform ER\n",
    "- Prioritized ER\n",
    "- Hindsight ER\n",
    "- Combined ER\n",
    "\n",
    "Our experiements are limited to a subset of the Open-AI Gym's Classic Control environments,\n",
    "- Cart Pole\n",
    "- Mountain Car\n",
    "- Acrobot\n",
    "\n",
    "Using these ER methods for Q-Learning in the Gym environments, we measure the influence of i.i.d. and TD-error to quantiatively compare the performances.\n",
    "\n",
    "Our hypothesis is that the effect of i.i.d. has a greater influence in the performance in comparision to the TD-error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "In this section we load the required libraries to implement Q-Learning and run the environments of Open-AI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Here, we define a Neural Network for the Q function. The forward pass accepts the observation state and outputs the action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=4, num_hidden=128, output_dim=2):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(input_dim, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l2(self.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "input_dim = 4\n",
    "num_hidden = 128\n",
    "output_dim = 2\n",
    "torch.manual_seed(1234)\n",
    "model = QNetwork(input_dim, num_hidden, output_dim)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(input_dim, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, output_dim)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# at this point we do not need backpropagation\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(model(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions for training,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(it):\n",
    "    if it == 0:\n",
    "        return 1\n",
    "    return np.maximum(1 - it/1000, 0.05)\n",
    "\n",
    "def select_action(model, state, epsilon):\n",
    "    with torch.no_grad():\n",
    "        actionProbs = model(torch.Tensor(state)).numpy()\n",
    "    action = np.random.choice(len(actionProbs), 1) if np.random.rand() < epsilon else np.argmax(actionProbs)\n",
    "    return int(action.squeeze()), actionProbs[0][action]\n",
    "\n",
    "def copy_model(source_model, target_model):\n",
    "    target_model.load_state_dict(source_model.state_dict())\n",
    "\n",
    "def append_sample(transition, model, target_model, discount_factor, memory):\n",
    "    state, action, reward, next_state, done = transition\n",
    "    target = model(torch.FloatTensor(state)).data\n",
    "    old_val = target[0][action].item()\n",
    "    target_val = target_model(torch.FloatTensor(next_state)).data\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + discount_factor * torch.max(target_val)\n",
    "    new_val = target[0][action].item()\n",
    "    error = abs(old_val - new_val)\n",
    "    transition = (state, action, reward, next_state, done)\n",
    "    memory.push(transition, error)\n",
    "    \n",
    "    \n",
    "def correlation_count(state, threshold=0.8):\n",
    "    cov=np.cov(state)\n",
    "    corrcoeff = cov.copy()\n",
    "    n_samples = cov.shape[0]\n",
    "    for i in range(n_samples):\n",
    "        corrcoeff[i,:]=corrcoeff[i,:]/np.sqrt(cov[i,i])\n",
    "    for j in range(n_samples):\n",
    "        corrcoeff[:,j]=corrcoeff[:,j]/np.sqrt(cov[j,j])\n",
    "    \n",
    "    return (np.sum(np.abs(corrcoeff)>threshold) - n_samples) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(memory, model, target_model, batch_size, action_size, optimizer, discount_factor):\n",
    "\n",
    "    if len(memory) < batch_size:\n",
    "        return None, None, None #To match length of output\n",
    "\n",
    "    transition_and_error, idxs, is_weights = memory.sample(batch_size)\n",
    "    \n",
    "    #Separate mini-batch into transition and TD error\n",
    "    mini_batch, sampled_TD_error = zip(*transition_and_error)    \n",
    "    sampled_TD_error = np.asarray(sampled_TD_error)\n",
    "    \n",
    "    #Set up data for training\n",
    "    mini_batch = np.array(mini_batch).transpose()\n",
    "    states = np.vstack(mini_batch[0])\n",
    "    actions = list(mini_batch[1])\n",
    "    rewards = list(mini_batch[2])\n",
    "    next_states = np.vstack(mini_batch[3])\n",
    "    dones = mini_batch[4]\n",
    "\n",
    "    #Collect stats\n",
    "    error_stats = (sampled_TD_error.mean(), np.median(sampled_TD_error), sampled_TD_error.max(), sampled_TD_error.std())\n",
    "    iid_stats = correlation_count(states)\n",
    "    \n",
    "    # bool to binary\n",
    "    dones = dones.astype(int)\n",
    "\n",
    "    # Q function of current state\n",
    "    states = torch.Tensor(states)\n",
    "    pred = model(states)\n",
    "\n",
    "    # one-hot encoding\n",
    "    a = torch.LongTensor(actions).view(-1, 1)\n",
    "\n",
    "    one_hot_action = torch.zeros(batch_size, action_size)\n",
    "    one_hot_action.scatter_(1, a, 1)\n",
    "\n",
    "    pred = torch.sum(pred.mul(torch.tensor(one_hot_action)), dim = 1)\n",
    "\n",
    "    # Q function of next state\n",
    "    next_states = torch.Tensor(next_states)\n",
    "    next_pred = target_model(next_states).data\n",
    "\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    dones = torch.FloatTensor(dones)\n",
    "\n",
    "    # Q Learning: get maximum Q value at s' from target model\n",
    "    target = rewards + (1 - dones) * discount_factor * next_pred.max(1)[0]\n",
    "    target = torch.autograd.Variable(target)\n",
    "\n",
    "    errors = torch.abs(pred - target).data.numpy()\n",
    "    \n",
    "    # update priority\n",
    "    if isinstance(memory, PrioritizedMemory):\n",
    "        for i in range(batch_size):\n",
    "            idx = idxs[i]\n",
    "            memory.update(idx, errors[i])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # MSE Loss function\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    loss.backward()\n",
    "\n",
    "    # and train\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), error_stats, iid_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(model, target_model, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    #initialize output\n",
    "    results_output = {}\n",
    "    results_output['durations'] = np.zeros(num_episodes)\n",
    "    results_output['mean_TD_error'] = np.zeros(num_episodes)\n",
    "    results_output['median_TD_error'] = np.zeros(num_episodes)\n",
    "    results_output['max_TD_error'] = np.zeros(num_episodes)\n",
    "    results_output['std_TD_error'] = np.zeros(num_episodes)\n",
    "    results_output['correlation_count'] = np.zeros(num_episodes)\n",
    "    results_output['Q_values'] = []\n",
    "    results_output['rewards'] = []\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        mean_TD_error = []\n",
    "        median_TD_error = []\n",
    "        max_TD_error = []\n",
    "        std_TD_error = []\n",
    "        correlation_count = []\n",
    "        Q_values = []\n",
    "        rewards = []\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            action, Q_val = select_action(model, state, get_epsilon(iteration))\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            Q_values.append(Q_val)\n",
    "            \n",
    "            # if an action make the episode end, then gives penalty of -10\n",
    "            reward = reward if not done or score == 499 else -10\n",
    "            rewards.append(reward)\n",
    "\n",
    "            transition = (state, action, reward, next_state, done)\n",
    "    \n",
    "            append_sample(transition, model, target_model, discount_factor, memory)\n",
    "    \n",
    "            # Train\n",
    "            loss, error_stats, iid_stats = train(memory, model, target_model, batch_size, action_size, optimizer, discount_factor)\n",
    "            \n",
    "            # Update\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            iteration += 1\n",
    "            \n",
    "            # Append stats\n",
    "            if error_stats != None:\n",
    "                results_output['durations'][episode] += 1\n",
    "                mean_TD_error.append(error_stats[0])\n",
    "                median_TD_error.append(error_stats[1])\n",
    "                max_TD_error.append(error_stats[2])\n",
    "                std_TD_error.append(error_stats[3])\n",
    "                correlation_count.append(iid_stats)\n",
    "\n",
    "            if done:\n",
    "                results_output['Q_values'].append(np.array([x for x in Q_values[:]]))\n",
    "                results_output['rewards'].append(np.array([x for x in rewards[:]]))\n",
    "                # every episode update the target model to be same with model\n",
    "                copy_model(model, target_model)\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 10\n",
    "                scores.append(score)\n",
    "                 \n",
    "        #Average stats per episode\n",
    "        if len(mean_TD_error) > 0:\n",
    "            results_output['mean_TD_error'][episode] = np.array(mean_TD_error).mean()\n",
    "            results_output['median_TD_error'][episode] = np.array(median_TD_error).mean()\n",
    "            results_output['max_TD_error'][episode] = np.array(max_TD_error).mean()\n",
    "            results_output['std_TD_error'][episode] = np.array(std_TD_error).mean()\n",
    "            results_output['correlation_count'][episode] = np.array(correlation_count).mean()\n",
    "                \n",
    "    return results_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "We load the necessary Gym environments required for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environments = [\"CartPole-v0\", \"Acrobot-v1\", \"MountainCar-v0\"]\n",
    "envs = {}\n",
    "\n",
    "print('Environment \\t\\t State Space \\t Action Space')\n",
    "print('----------- \\t\\t ----------- \\t ------------')\n",
    "for environment in environments:\n",
    "    env = gym.envs.make(environment)\n",
    "    envs[environment] = env\n",
    "    print('{} \\t\\t {} \\t {}'.format(\n",
    "        environment,\n",
    "        env.observation_space,\n",
    "        env.action_space)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition, error):\n",
    "        self.memory.append((transition, error))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        raise NotImplementedError('Function sample is not defined')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Experience Replay and Uniform Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoneMemory(ReplayMemory):\n",
    "    def sample(self, batch_size):\n",
    "        #Samples a tuple of tuple of transitions and error - ((s, a, r, s', done), TDerror)\n",
    "        return self.memory[-batch_size:], None, None # extra Nones to have the same behaviour as PER\n",
    "\n",
    "\n",
    "class UniformMemory(ReplayMemory):\n",
    "    def sample(self, batch_size):\n",
    "        #Samples a tuple of tuple of transitions and error - ((s, a, r, s', done), TDerror)\n",
    "        index = np.random.choice(len(self), batch_size, replace=False)\n",
    "        return [self.memory[i] for i in index], None, None # extra Nones to have the same behaviour as PER\n",
    "\n",
    "\n",
    "class CombinedMemory(ReplayMemory):\n",
    "    def sample(self, batch_size):\n",
    "        #Samples a tuple of tuple of transitions and error - ((s, a, r, s', done), TDerror)\n",
    "        index = np.random.choice(len(self) - 1, batch_size - 1, replace=False)\n",
    "        samples = [self.memory[i] for i in index]\n",
    "        samples.append(self.memory[-1])\n",
    "        return samples, None, None # extra Nones to have the same behaviour as PER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rlcode/per\n",
    "\n",
    "# SumTree\n",
    "# a binary tree data structure where the parent’s value is the sum of its children\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype = object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "\n",
    "class PrioritizedMemory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "    def push(self, sample, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, (sample, error))\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "        \n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the memory module works as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity = 10\n",
    "memory = NoneMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done), 0)\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the train module works as expected,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "\n",
    "model = QNetwork(input_dim, num_hidden, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "\n",
    "# setup memory\n",
    "transition = memory.sample(1)[0][0][0]\n",
    "memory = UniformMemory(10 * batch_size)\n",
    "\n",
    "# fill with dummy data\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition, 0)\n",
    "\n",
    "# must prints loss, meanTDerror, (medianTDerror, maxTDerror, stdTDerror), Correlation count\n",
    "print(train(memory, model, model, batch_size, output_dim, optimizer, discount_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(er_type='uniform', environment=\"CartPole-v0\"):\n",
    "\n",
    "    # load environment\n",
    "    env = envs[environment]\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    num_hidden = 128\n",
    "\n",
    "    # init networks\n",
    "    model = QNetwork(state_size, num_hidden, action_size)\n",
    "    target_model = QNetwork(state_size, num_hidden, action_size)\n",
    "\n",
    "    copy_model(model, target_model)\n",
    "\n",
    "    # run settings\n",
    "    batch_size = 64\n",
    "    discount_factor = 0.8\n",
    "    learn_rate = 1e-3\n",
    "    memory_size = 10000\n",
    "    num_episodes = 300\n",
    "    \n",
    "    er_types = {\n",
    "        'none': NoneMemory,\n",
    "        'uniform': UniformMemory,\n",
    "        'prioritized': PrioritizedMemory,\n",
    "        'combined': CombinedMemory\n",
    "    }\n",
    "\n",
    "    memory = er_types[er_type](memory_size)\n",
    "\n",
    "    #Return a dictionary of the relevant statistics per episode\n",
    "    metrics = run_episodes(model, target_model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_replays = ['none', 'uniform', 'prioritized', 'combined']\n",
    "environments = ['CartPole-v0', 'Acrobot-v1', 'MountainCar-v0']\n",
    "\n",
    "ITERATIONS = 50\n",
    "\n",
    "def grid_search(iterations):\n",
    "    results = nested_dict()\n",
    "    for environment in environments:\n",
    "        for er_type in exp_replays:\n",
    "            for iteration in range(iterations):\n",
    "                print('Running {} in {}'.format(er_type, environment))\n",
    "                results[environment][er_type][iteration] = run(er_type, environment)\n",
    "    return results\n",
    "\n",
    "def save(data):\n",
    "    with open('results.picke', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# use results and then next cell for plotting - no need to get results multiple times\n",
    "results = grid_search(100)\n",
    "# results = run('prioritized', 'Acrobot-v1')\n",
    "save(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smooth_range = 10\n",
    "\n",
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "def plot_Q_values(Q_values, spacing):\n",
    "    for i in range(0, len(Q_values), spacing):\n",
    "        plt.subplot(len(Q_values)/spacing,1, i/spacing+1)\n",
    "        plt.plot(Q_values[i])\n",
    "        plt.title('Q_values in episode {} '.format(i))\n",
    "    # plt.subplots_adjust(top=1, bottom=-1.5)\n",
    "    plt.show()\n",
    "\n",
    "def grid_graphs():\n",
    "    # plot durations\n",
    "    # compare per environment\n",
    "    for index, env in enumerate(environments):\n",
    "        plt.subplot(len(environments),1, index+1)\n",
    "        for er_type in exp_replays:\n",
    "            plt.plot(smooth(results[env][er_type]['durations'], smooth_range))\n",
    "        plt.title('Episode durations per episode in {} environment'.format(env))\n",
    "        plt.legend(exp_replays)\n",
    "    # plt.subplots_adjust(top=1, bottom=-1.5)\n",
    "    plt.show()\n",
    "    \n",
    "    # compare per ER\n",
    "    for index, er_type in enumerate(exp_replays):\n",
    "        plt.subplot(len(exp_replays),1, index+1)\n",
    "        for env in environments:\n",
    "            plt.plot(smooth(results[env][er_type]['durations'], smooth_range))\n",
    "        plt.title('Episode durations per episode with {} experience replay'.format(exp_replays[index]))\n",
    "        plt.legend(environments)\n",
    "    plt.subplots_adjust(top=1, bottom=-1.5)\n",
    "    plt.show()\n",
    "    \n",
    "# grid_graphs()\n",
    "# print(results[\"CartPole-v0\"])\n",
    "plot_Q_values(results['Q_values'], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
